# R-News 
one of the main drawbacks of the first generation of Transformers and
BERT based architectures; the sequence length is limited to a maximum
of 512 characters. The reason behind that limitation is the fact that
self-attention mechanism scales quadratically with the input sequence
length O(n^2).
Uses Efficient sparse attention, which makes it faster than traditional 
transformers.

Using AMP [Research]

Using Captum [Research]